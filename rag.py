import os 
from langchain_chroma import Chroma
from vector_db import create_vector_store
from langchain_huggingface import HuggingFaceEmbeddings
#from langchain_huggingface import HuggingFacePipeline
from langchain_ollama.llms import OllamaLLM
from langchain_core.tools import tool 
from langchain_tavily import TavilySearch
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
from tavily import TavilyClient


load_dotenv()  # Load environment variables from .env file

# Set up the embedding model
embedding_function = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# Load vector store if it exists, otherwise create it
if os.path.exists("./rag_db"):
    vectorstore = Chroma(persist_directory="./rag_db",
                         embedding_function=embedding_function
    )
else:
    print("Vector store not found, creating a new one...")

    # Create vector store if it doesn't exist
    create_vector_store(
        docs_path="./docs",
        embedding_model="BAAI/bge-m3",
        chunk_size=2000,
        chunk_overlap=300
    )

    vectorstore = Chroma(persist_directory="./rag_db",
                    embedding_function=embedding_function)
    
retriever = vectorstore.as_retriever(
    search_type = "similarity_score_threshold",
    search_kwargs = {"score_threshold": 0.40,
                    "k": 2}
)

# Function to load the LLM model
def load_llm_model():
    llm = OllamaLLM(
        model="llama3.2",
        temperature=0.3,        # less randomness, more factual/polite
        top_p=0.9,              # control diversity
        top_k=50,               # limit vocabulary sampling
        repeat_penalty=1.1,     # discourage repeating same phrases
        num_predict=512         # max tokens to generate
    )
    return llm

# Load LLM model
llm = load_llm_model()

# RAG tool
@tool
def rag_tool(query: str) -> str:

    '''
    This tool retrieves relevant documents from the vector store and generates a response using the LLM.
    If no relevant documents are found, we search the web for relevant information.

    :param query: Query to process.
    :return: Response generated by the LLM based on retrieved documents or relevant information from the web.

    '''
    rag_template = """
    You are ChatFood AI, a knowledgeable assistant specializing in food, cuisine, and restaurants.

    You will be given a **context** from the knowledge base and a **user question**.
    Your job is to:
    1. Answer **only** based on the provided context.
    2. Be concise, clear, and helpful. Add extra useful details from the context if they help the user.
    3. Format the answer in an easy-to-read style. Use bullet points or short paragraphs if appropriate.

    -----------------
    CONTEXT:
    {context}

    QUESTION:
    {query}

    FINAL ANSWER:
    """
    prompt_template = PromptTemplate.from_template(template=rag_template)

    # Retrieve relevant documents from the vector store
    docs = retriever.invoke(query)

    if not docs or len(docs) == 0:
        print("The information is not available. Searching the web ...")
            
        tavily_client = TavilyClient()
        tavily_data = tavily_client.search(
        query=query,
        search_depth="basic",
        max_results=3
        )

        # Extract clean text from results
        tavily_results = "\n\n".join(res["content"] for res in tavily_data["results"])

        chain = prompt_template | llm
        
        response = chain.invoke({"query": query, "context": tavily_results})
        return response
    
    else:
        # if relevant documents are found, generate a response using the LLM
        context = "\n\n".join([doc.page_content for doc in docs])
        chain = prompt_template | llm
        response = chain.invoke({"query": query, "context": context})
        return response



# Test
if __name__ == "__main__":

    #os.environ["TAVILY_API_KEY"] = os.getenv("TAVILY_API_KEY")

    #query = "What is the best football player in the world?"
    #query = "What is pizza?"
    query = "Waht is the nutritional Profile of carrot?"
    #query = "What is carrot?"

    response = rag_tool(query)
    print(response)